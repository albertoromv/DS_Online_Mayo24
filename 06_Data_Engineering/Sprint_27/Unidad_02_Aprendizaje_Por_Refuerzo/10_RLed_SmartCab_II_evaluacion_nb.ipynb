{"cells":[{"cell_type":"markdown","metadata":{},"source":["![logo](./img/TheBridge_RL.png)"]},{"cell_type":"markdown","metadata":{},"source":["# Taxi Autónomo (Smartcab), CON RL\n","### *Segunda Parte: Evaluación*"]},{"cell_type":"markdown","metadata":{},"source":["## Contenidos"]},{"cell_type":"markdown","metadata":{},"source":["* [Inicializacion y setup](#Inicializacion-y-setup)  \n","\n","* [Evaluacion](#Evaluacion)  \n","\n","* [Comparando nuestro agente de Q-learning con no usar Aprendizaje por Refuerzo](#Comparando-nuestro-agente-de-Q-learning-con-no-usar-Aprendizaje-por-Refuerzo)  \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Inicializacion y setup\n","  \n","\n","[al indice](#Contenidos)  \n","\n","Necesarios para volver al punto donde lo dejamos en la primera parte, ejecuta las celdas hasta llegar a la cabecera: \"Evaluación\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gym\n","import warnings\n","\n","\n","env = gym.make(\"Taxi-v3\", render_mode = \"ansi\").env"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env.reset(seed = 19)\n","print(env.render())\n","print(\"Current State:\", env.s)\n","print(\"Action Space {}\".format(env.action_space))\n","print(\"State Space {}\".format(env.observation_space))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["movements = [2,0]\n","for mov in movements:\n","    env.step(mov)\n","    print(env.render())\n","    print(\"State:\",env.s)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from time import sleep\n","from IPython.display import clear_output\n","\n","def episode_animation(frames):\n","    for i, frame in enumerate(frames): # Recorremos todo el conjunto de frames\n","        clear_output(wait=True) # Limpiamos la \"pantalla\"\n","        print(frame['frame']) # Visualizamos el \"pantallazo\" resultado de cada acción\n","        print(f\"Timestep: {i + 1}\") # Aumentamos el contador de pasos/steps\n","        # Imprimimos el resto de valores correspondientes a cada frame y que hemos guardado al realizar el \"aprendizaje\"\n","        print(f\"State: {frame['state']}\") \n","        print(f\"Action: {frame['action']}\")\n","        print(f\"Reward: {frame['reward']}\")\n","        print(f\"Elapsed time (sec.): {frame['elapsed']}\")\n","        sleep(.1) # \"Dormimos\" el programa un tiempo para que nuestro ojo pueda ver la imagen antes de borrarla y mostrar la siguiente"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["alpha = 0.05\n","gamma = 0.9\n","epislon = 0.1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import random\n","\n","all_epochs = []\n","all_penalties = []\n","num_episodes = 100000 \n","state = env.s\n","\n","for i in range(1, num_episodes +1): \n","    epochs, penalties, reward = 0,0,0\n","    done = False\n","    \n","    while not done: \n","        if random.uniform(0,1) < epislon:\n","            action = env.action_space.sample()\n","        else:\n","            action = np.argmax(q_table[state])\n","        next_state, reward, done, truncated, info = env.step(action)\n","        old_value = q_table[state,action]\n","        next_max = np.max(q_table[next_state]) # maxQ(s',a')\n","        \n","        new_value = (1 - alpha)*old_value + alpha * (reward + gamma * next_max)\n","        \n","        q_table[state,action] = new_value\n","        \n","        if reward == -10:\n","            penalties += 1\n","            \n","        state = next_state\n","        epochs += 1\n","        \n","    if i % 100 == 0:\n","        clear_output(wait = True)\n","        print(f\"Episode: {i}, {i/num_episodes * 100:.2f}%\")\n","    state,info = env.reset()\n","    \n","print(\"Entrenamiento finalizado\")\n","    "]},{"cell_type":"markdown","metadata":{},"source":["### Evaluacion  \n","\n","[al indice](#Contenidos)  \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Vamos a evaluar el rendimiento de nuestro agente. Ya no necesitamos explorar acciones, así que ahora la siguiente acción siempre se selecciona usando el mejor valor Q:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["# NO EJECUTES EL CODIGO DIRECTAMENTE PORQUE SE ENCARGA DE CALCULAR Q\n","# HAY QUE MODIFICARLO PRIMERO PARA QUE REALMENTE USE LA TABLA Q\n","\n","all_epochs = []\n","all_penalties = []\n","num_episodes = 100000 \n","state = env.s\n","\n","for i in range(1, num_episodes +1): \n","    epochs, penalties, reward = 0,0,0\n","    done = False\n","    \n","    while not done: \n","        if random.uniform(0,1) < epislon:\n","            action = env.action_space.sample()\n","        else:\n","            action = np.argmax(q_table[state])\n","        next_state, reward, done, truncated, info = env.step(action)\n","        old_value = q_table[state,action]\n","        next_max = np.max(q_table[next_state]) # maxQ(s',a')\n","        \n","        new_value = (1 - alpha)*old_value + alpha * (reward + gamma * next_max)\n","        \n","        q_table[state,action] = new_value\n","        \n","        if reward == -10:\n","            penalties += 1\n","            \n","        state = next_state\n","        epochs += 1\n","        \n","    if i % 100 == 0:\n","        clear_output(wait = True)\n","        print(f\"Episode: {i}, {i/num_episodes * 100:.2f}%\")\n","    state,info = env.reset()"]},{"cell_type":"markdown","metadata":{},"source":["Los valores parecen bastante buenos, 12.5 pasos de duración media y no hay penalizaciones (no se intenta recoger o dejar al pasajero en sitios equivocados)"]},{"cell_type":"markdown","metadata":{},"source":["Utilicemos ahora la visualización para ver cuanto de bien ha aprendido a conducir. Vamos a analizar 5 episodios escogidos aleatoriamente."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["Bastante bien, ¿no? Comparemos ahora con el \"entrenamiento\", por llamarlo de alguna forma, sin aprendizaje por refuerzo"]},{"cell_type":"markdown","metadata":{},"source":["### Comparando nuestro agente de Q-learning con no usar Aprendizaje por Refuerzo\n","  \n","\n","[al indice](#Contenidos)  \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Vamos a evaluar a nuestros agentes de acuerdo con las siguientes métricas,\n","\n","* Número promedio de penalizaciones por episodio: Cuanto menor sea el número, mejor será el rendimiento de nuestro agente. Idealmente, nos gustaría que esta métrica sea cero o muy cercana a cero.\n","* Número promedio de pasos por episodio: También queremos que sea un valor pequeño, que nuestro agente tome la ruta más corta para llegar al destino.\n","* Recompensas promedio por movimiento: Una recompensa más grande significa que el agente está haciendo lo correcto. Es por eso que decidir las recompensas es una parte crucial del Aprendizaje por Refuerzo.\n"]},{"cell_type":"markdown","metadata":{},"source":["Recuperemos el código que ya desarrollamos en la sesión sin aprendizaje por refuerzo para obtener los valores anteriores para este escenario y hacer la comparativa"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"Evaluate agent's performance without Q-learning\"\"\"\n","\n","total_epochs, total_penalties, total_rewards = 0, 0, 0\n","episodes = 100\n","\n","for _ in range(episodes):\n","    env.reset()\n","    # Crea el estado inicial\n","    state = env.encode(3, 1, 2, 0)\n","    env.s = state\n","    # Inicializa las epochs, penalties y rewards\n","    epochs, penalties, reward = 0, 0, 0\n","\n","    done = False\n","    actions = []\n","    while not done:\n","        # Elige la acción random\n","        action = env.action_space.sample()\n","        actions.append(action)\n","        # Ejecuta la accion\n","        state, reward, done, truncated, info = env.step(action)\n","        total_rewards += reward\n","        # Actualiza el valor de penalties si el reward es -10\n","        if reward == -10:\n","            penalties += 1\n","\n","        epochs += 1\n","\n","    total_penalties += penalties\n","    total_epochs += epochs\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average penalties per episode: {total_penalties / episodes}\")\n","print(f\"Average reward per step: {total_rewards/total_epochs}\")"]},{"cell_type":"markdown","metadata":{},"source":["Rellena la tabla y observa la comparación"]},{"cell_type":"markdown","metadata":{},"source":["| **Medida**              | **Rendimiento Agente Aleatorio** | **Rendimiento Agente Q-Learning** |\n","|---------------------------|-----------------------------|-------------------------------|\n","| Average penalties per episode                   |                  ||\n","| Average timesteps per episode                   |              |                     |\n","| Average reward per action|        |                  |"]},{"cell_type":"markdown","metadata":{},"source":["Estas métricas se calcularon a lo largo de 100 episodios. ¡Y como muestran los resultados, nuestro agente de Q-learning lo clavó!\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":2}
