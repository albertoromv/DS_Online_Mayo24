{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["![logo](./img/TheBridge_RL.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Introducci\u00f3n a los entornos de prueba de aprendizaje por refuerzo (el Gym) (I)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Contenidos"]}, {"cell_type": "markdown", "metadata": {}, "source": ["* [Motivacion](#Motivacion)  ", "\n", "* [Gym de OpenAI](#Gym-de-OpenAI)  ", "\n", "* [Primeros pasos con Gym](#Primeros-pasos-con-Gym)  ", "\n", "* [Steps, Episodes y Actions (pasos, episodios y acciones)](#Steps,-Episodes-y-Actions-(pasos,-episodios-y-acciones))  ", "\n", "* [Visualizacion (*Rendering*)](#Visualizacion-(*Rendering*))  ", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Motivacion\n  ", "\n", "[al indice](#Contenidos)  ", "\n", "Uno de los elementos principales de un sistema de aprendizaje por refuerzo es el entorno. Por tanto, tendremos que disponer de \u00e9l.  \n", "\n", "Podr\u00eda pensarse que cuanto m\u00e1s real, mejor, pero los entornos reales tienen tambi\u00e9n serias limitaciones.  \n", "\n", "Considera el caso de que queramos entrenar un robot de rescate en la monta\u00f1a. Podemos pensar en entrenarlo en una monta\u00f1a de verdad, pero:\n", "* Si el robot se cae por una ladera no hay forma de hacer Ctrl+Z.\n", "* Estamos limitados al tiempo real, es decir si queremos que el robot aprenda en un recorrido de 100Km, cada intento llevar\u00e1 el tiempo que le lleve al robot recorrer 100Km. No podemos acelerar el tiempo.\n", "\n", "Por eso, empleamos simuladores de entornos y por sus caracter\u00edsticas, reglas y acciones concretas, que facilitan su simulaci\u00f3n, se emplean videojuegos."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Gym de OpenAI  ", "\n", "[al indice](#Contenidos)  ", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Dentro de los posibles simuladores de entornos el m\u00e1s conocido es Gym(*) de OpenAI. Este simulador permite testar, desarrollar y comparar algoritmos de aprendizaje por refuerzo (desde los cl\u00e1sicos juegos de Atari a entornos \"reales\" en 2D y 3D, pasando por varios juegos de mesa).\n", "\n", "Uno de los puntos fuertes es que se emplea una interfaz \u00fanica para todos los entornos, lo que permite escribir algoritmos gen\u00e9ricos. S\u00f3lo necesitaras cambiar una l\u00ednea de c\u00f3digo para probarlo en cualquiera de los entornos disponibles.   \n", "\n", "Comencemos!!!\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Primeros pasos con Gym  ", "\n", "[al indice](#Contenidos)  ", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lo primero es importar el m\u00f3dulo"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Para cargar un entorno s\u00f3lo tienes que teclear lo siguiente, indicando el nombre del entorno que se quiera simular"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["En este caso estamos instanciando un entorno de un juego cuyo objetivo es mover un vag\u00f3n en el fondo de un valle de forma que alcance la parte superior de una colina."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src= \"./img/mountain_car_example.png\" width = 500 />"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Si queremos ver todos los entornos disponibles s\u00f3lo tenemos que tecelar lo siguiente"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Una vez cargado un entorno, lo primero que hacemos es inicializarlo:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Y nos devuelve un array con las observaciones, e informaci\u00f3n adicional. \n", "En este entorno, las observaciones son la posici\u00f3n en el eje-x, y la velocidad del vag\u00f3n."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Steps, Episodes y Actions (pasos, episodios y acciones)  ", "\n", "[al indice](#Contenidos)  ", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["En el contexto del aprendizaje por refuerzo, nos quedan dos conceptos por conocer:\n", "- Step (Paso): La ejecuci\u00f3n de una acci\u00f3n de un agente sobre un entorno que hace que este devuelva las observaciones del nuevo estado y la recompensa\n", "- Episode (Episodio): Conjunto completo de pasos (steps) que conforman un ciclo completo de entrenamiento, lo que viene a ser una \"partida\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["En el caso de Gym, la forma de ejecutar un step, es la siguiente:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["\u00bfY qu\u00e9 acciones?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Nos dice que hay tres acciones, discretas, es decir, accion 0, acci\u00f3n 1, y acci\u00f3n 2\n", "Si consultamos la documentaci\u00f3n de OpenAI Gym:\n", "  \n", "0: Acelerar a la izquierda  \n", "1: No hacer nada  \n", "2: Acelerar a la derecha  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Si queremos m\u00e1s informaci\u00f3n, en la misma documentaci\u00f3n podemos ver que f\u00f3rmula sigue el entorno para simular la aceleraci\u00f3n o, importante cual es el criterio de recompensa (en este caso castigo):\n", "- Como el objetivo es llegar a la cima lo antes posible, a cada acci\u00f3n le corresponde siempre una recompensa de -1. Este es un caso en el que querremos minimizar el reward de cada episodio."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora que conocemos las acciones, ejecutemos un step como si fueramos el agente:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Nos ha devuelto cinco valores:\n", "* __las observaciones__ del nuevo estado, es decir la posici\u00f3n en el eje-x y la velocidad con su signo (como hemos indicado que acelere a la izquierda y estaba quieto ha cogido velocidad \"negativa\", hacia la izquierda)\n", "* __el reward__ que como hemos comentado siempre es -1\n", "* __un flag de terminated__ nos indica si el episodio se ha terminado porque se ha alcanzado alguna de las condiciones de parada (en nuestro caso si se ha llegado a la cima)\n", "* __un flag de truncado__ es un flag especial por si hay problemas con el entorno y debe reinicializarse\n", "* __informaci\u00f3n adicional__ que se emplea en algunos entornos, como por ejemplo los juegos de Atari para indicarnos las vidas que le quedan al agente"]}, {"cell_type": "markdown", "metadata": {}, "source": ["T\u00edpicamente, ejecutaremos el step almacenando su salida en diferentes variables, algo como:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Visualizacion (*Rendering*)  ", "\n", "[al indice](#Contenidos)  ", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Con los comandos vistos ya podr\u00edamos programar un ciclo completo de entrenamiento de aprendizaje por refuerzo, pero de una forma bastante \u00e1rida y con poca capacidad por parte de un ser humano de poder \"ver\" c\u00f3mo est\u00e1 evolucionando."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Lo que nos ha devuelto es el contenido de la pantalla pixel a pixel que corresponde al estado generado por nuestra acci\u00f3n. Esto es as\u00ed porque cuando creamos el entorno utilizamos el argumento render_mode con el valor \"rgb_array\":   \n", "```python\n", "env = gym.make(\"MountainCar-v0\", render_mode = \"rgb_array\")\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Vamos a emplear un m\u00e9todo que utiliza una librer\u00eda como matplotlib que funciona con cualquier sistema operativo"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Ejecutemos varios steps seguidos acelerando hacia la izquierda y veamos el resultado final"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Ahora t\u00fa, carga otros entornos, busca documentaci\u00f3n en https://gymnasium.farama.org/, y prueba \"los aparatos\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "**IMPORTANTE**: No todos los entornos se pueden mostrar (renderizar) de forma directa en todos los sistemas operativos, tendr\u00e1s que mirarlo con un poquito de cuidado\n", "\n", "**CONSEJO**: Descargate este notebook y juega con \u00e9l en tu ordenador, el entorno remoto est\u00e1 restringido en su capacidad para las sesiones online"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**INSTALACION**: Gym como tal dej\u00f3 de ser evolucionado por OpenAI a finales del a\u00f1o pasado, pero todav\u00eda est\u00e1 operativo. La alternativa es gymnasium. Te dejo aqu\u00ed los links a ambos y ojo mira la documentaci\u00f3n por si te lo quieres instalar con diferentes entornos a los que vienen con la configuraci\u00f3n b\u00e1sica (la de hacer pip install gym, o pip install gymnasium)\n", "\n", "[Al Gym](https://www.gymlibrary.dev/index.html)  \n", "\n", "[Al Gymnasium](https://gymnasium.farama.org/)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "nbformat": 4, "nbformat_minor": 2, "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.4"}}}