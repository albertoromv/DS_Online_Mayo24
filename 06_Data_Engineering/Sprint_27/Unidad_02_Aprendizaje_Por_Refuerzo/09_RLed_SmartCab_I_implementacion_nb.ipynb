{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["![logo](./img/TheBridge_RL.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Taxi Aut\u00f3nomo (Smartcab), CON RL\n", "### *Primera parte: Implementaci\u00f3n*"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Contenidos"]}, {"cell_type": "markdown", "metadata": {}, "source": ["* [Inicializacion](#Inicializacion)  ", "\n", "* [Q-Learning](#Q-Learning)  ", "\n", "* [A programar...](#A-programar...)  ", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Inicializacion  ", "\n", "[al indice](#Contenidos)  ", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import gym\n", "import warnings\n", "\n", "\n", "env = gym.make(\"Taxi-v3\", render_mode = \"ansi\").env"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env.reset(seed = 19)\n", "print(env.render())\n", "print(\"Current State:\", env.s)\n", "print(\"Action Space {}\".format(env.action_space))\n", "print(\"State Space {}\".format(env.observation_space))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["movements = [2,0]\n", "for mov in movements:\n", "    env.step(mov)\n", "    print(env.render())\n", "    print(\"State:\",env.s)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from time import sleep\n", "from IPython.display import clear_output\n", "\n", "def episode_animation(frames):\n", "    for i, frame in enumerate(frames): # Recorremos todo el conjunto de frames\n", "        clear_output(wait=True) # Limpiamos la \"pantalla\"\n", "        print(frame['frame']) # Visualizamos el \"pantallazo\" resultado de cada acci\u00f3n\n", "        print(f\"Timestep: {i + 1}\") # Aumentamos el contador de pasos/steps\n", "        # Imprimimos el resto de valores correspondientes a cada frame y que hemos guardado al realizar el \"aprendizaje\"\n", "        print(f\"State: {frame['state']}\") \n", "        print(f\"Action: {frame['action']}\")\n", "        print(f\"Reward: {frame['reward']}\")\n", "        print(f\"Elapsed time (sec.): {frame['elapsed']}\")\n", "        sleep(.1) # \"Dormimos\" el programa un tiempo para que nuestro ojo pueda ver la imagen antes de borrarla y mostrar la siguiente"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Q-Learning  ", "\n", "[al indice](#Contenidos)  ", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Recordemos brevemente los pasos del algoritmo de Q-Learning epsilo-greedy que nos permitir\u00e1 estimar la Q-table"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"./img/q-matrix-initialized-to-learned_gQq0BFs.png\" alt=\"drawing\" width=\"650\"/>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Desglos\u00e1ndolo en pasos, obtenemos:\n", "\n", "* Inicializar la tabla Q con todos ceros.\n", "* Seleccionar los valores de los hiperpar\u00e1metros\n", "* Comenzar a explorar acciones: Para cada estado, seleccione cualquiera entre todas las acciones posibles para el estado actual (S).\n", "* Viajar al siguiente estado (S') como resultado de esa acci\u00f3n (a).\n", "* Para todas las acciones posibles desde el estado (S') seleccione la que tenga el valor Q m\u00e1s alto.\n", "* Actualizar los valores de la tabla Q usando la ecuaci\u00f3n ya vista.\n", "* Establecer el siguiente estado como el estado actual.\n", "* Si se alcanza el estado objetivo, entonces terminar y repetir el proceso.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### A programar...  ", "\n", "[al indice](#Contenidos)  ", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lo primero creamos la estructura de datos que nos permita almacenar la Q-table"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Lo siguiente es seleccionar los valores de los hiperpar\u00e1metros, alpha, gamma y \u00e9psilon"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora podemos crear el algoritmo de entrenamiento que actualizar\u00e1 esta tabla Q a medida que el agente explore el entorno a lo largo de miles de episodios.\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora que hemos estimado la tabla Q tras los 100,000 episodios, veamos cu\u00e1les son los valores Q en el estado de nuestra ilustraci\u00f3n, que recordemos es el correspondiente al \u00edndice 328\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": []}], "nbformat": 4, "nbformat_minor": 2, "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.4"}}}