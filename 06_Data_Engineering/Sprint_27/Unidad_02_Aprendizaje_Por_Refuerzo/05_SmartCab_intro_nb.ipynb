{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["![logo](./img/TheBridge_RL.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Taxi Aut\u00f3nomo (Smartcab)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Contenidos"]}, {"cell_type": "markdown", "metadata": {}, "source": ["* [Objetivo](#Objetivo)  ", "\n", "* [Recompensas (Rewards)](#Recompensas-(Rewards))  ", "\n", "* [Espacio de Estados (State Space)](#Espacio-de-Estados-(State-Space))  ", "\n", "* [Action space](#Action-space)  ", "\n", "* [Implementacion](#Implementacion)  ", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Objetivo\n  ", "\n", "[al indice](#Contenidos)  ", "\n", "\n", "El trabajo del Smartcab es recoger al pasajero en un lugar y dejarlo en otro. Algunos detalles que nos encantar\u00eda que nuestro Smartcab tenga en cuenta ser\u00edan:\n", "\n", "* Dejar al pasajero en la ubicaci\u00f3n correcta.  \n", "* Ahorrar tiempo al pasajero dedicando el m\u00ednimo tiempo posible para dejarlo.  \n", "* Cuidar la seguridad del pasajero y respetar las normas de tr\u00e1fico.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Recompensas (Rewards)  ", "\n", "[al indice](#Contenidos)  ", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["* El agente deber\u00eda recibir una alta recompensa positiva por una entrega del cliente exitosa porque este comportamiento es de los m\u00e1s importantes que queremos que aprenda.\n", "* El agente deber\u00eda ser penalizado si intenta dejar a un pasajero en destinos incorrectos.\n", "* El agente deber\u00eda recibir una ligera recompensa negativa por no llegar a destino despu\u00e9s de cada intervalo de tiempo. \"Ligera\" negativa porque preferir\u00edamos que nuestro agente llegue tarde en lugar de hacer movimientos err\u00f3neos tratando de llegar al destino lo m\u00e1s r\u00e1pido posible.\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Para nuestro peque\u00f1o ejercicio vamos a establecer las siguientes \"recompensas\":\n", "* Recibimos +20 puntos por un traslado exitoso.  \n", "* Perdemos 1 punto por cada intervalo de tiempo que tarda.  \n", "* Tambi\u00e9n hay una penalizaci\u00f3n de 10 puntos por acciones de recogida y dejada ilegales. \n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Espacio de Estados (State Space)  ", "\n", "[al indice](#Contenidos)  ", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["El __Espacio de Estados__ es el conjunto de todas las posibles situaciones en las que nuestro taxi podr\u00eda estar. El estado adem\u00e1s debe contener informaci\u00f3n \u00fatil que el agente necesite para tomar la acci\u00f3n correcta."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src=\"./img/Reinforcement_Learning_Taxi_Env.png\" alt=\"drawing\" width=\"600\"/>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Supongamos que Smartcab es el \u00fanico veh\u00edculo en este circuito de aprendizaje. Nuestro circuito est\u00e1 dividido en __una cuadr\u00edcula de 5x5__, lo que nos da __25 posibles ubicaciones__ para el taxi (posiciones (0,0) a (5,5)). Estas 25 ubicaciones __son una parte de nuestro espacio de estados__. Observa que la __ubicaci\u00f3n actual__ de nuestro taxi es la __coordenada (3, 1)__.\n", "\n", "Tambi\u00e9n puedes ver que hay __cuatro (4) ubicaciones__ en las que podemos __recoger y dejar__ a un pasajero: R, G, Y, B o [(0,0), (0,4), (4,0), (4,3)] en coordenadas (fila, columna).  \n", "\n", "Tambi\u00e9n debemos tener en cuenta un (1) estado adicional del pasajero de estar dentro del taxi.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u00bfc\u00f3mo codificar\u00edamos el estado anterior de la forma m\u00e1s compacta:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Teniendo en cuenta lo anterior, \u00bfcu\u00e1ntos estados posibles hay en nuestro State Space?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["El entorno nos devolver\u00e1 un \u00edndice entre 0 y 500 para representar el estado al que se llega despu\u00e9s de ejecutar una acci\u00f3n (invocando el m\u00e9todo step())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Extra: \n", "* Si quisieramos generalizar y tener como destino y origen cualquiera de las cuadr\u00edculas de nuestro circuito, \u00bfcu\u00e1ntos estados tendr\u00eda nuestro espacio de estados?\n", "* Considerando los 500 estados que tenemos, \u00bfcu\u00e1ntos realmente se pueden visitar por partida? (Pista: Si al inicializar el entorno el pasajero est\u00e1 ya en su destino, \u00bfqu\u00e9 ocurre?)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Action space  ", "\n", "[al indice](#Contenidos)  ", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["El agente se encuentra con uno de los 500 estados y toma una acci\u00f3n. La acci\u00f3n en nuestro caso puede ser moverse en una direcci\u00f3n o decidir recoger/dejar a un pasajero.\n", "\n", "En otras palabras, tenemos seis posibles acciones:\n", "1. sur\n", "2. norte\n", "3. este\n", "4. oeste\n", "5. recoger\n", "6. dejar\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Seg\u00fan la ilustraci\u00f3n, el taxi no puede realizar ciertas acciones en ciertos estados debido a las paredes (por ejemplo mover de la posici\u00f3n (3,1) a la (3,0)).  \n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<img src = \"./img/Reinforcement_Learning_Taxi_Env.png\" />"]}, {"cell_type": "markdown", "metadata": {}, "source": ["El entorno proporciona una penalizaci\u00f3n de -1 por cada movimiento no permitido y el taxi no se mover\u00e1 a ning\u00fan lado. Como siguiente estado devuelve el mismo de partida."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Implementacion  ", "\n", "[al indice](#Contenidos)  ", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["* El cuadrado relleno representa el taxi, que es de color amarillo sin un pasajero y verde con un pasajero. En este caso, comenzar\u00edamos con nuestro taxi situado en (2,0).\n", "* La barra (\"|\") representa una pared que el taxi no puede cruzar.\n", "* R, G, Y, B son las posibles ubicaciones de recogida y destino. La letra azul representa la ubicaci\u00f3n actual de recogida del pasajero, y la letra morada es el destino actual. Es decir hay que recogerlo de Y y entregarlo en R, para este ejemplo generado al resetear el entorno con la semilla ajustada al valor 19."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Si quisieramos camibar la posici\u00f3n de partida de nuestro taxi y llevarlo a la de la figura, podemos moverlo con las acciones e ignorar las recompensas. Bastar\u00eda con moverlo al este y luego al sur."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "nbformat": 4, "nbformat_minor": 2, "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.4"}}}