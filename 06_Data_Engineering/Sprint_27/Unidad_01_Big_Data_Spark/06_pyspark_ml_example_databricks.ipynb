{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbca5a5c-0e53-412d-b818-ea7d26f6dc9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introducción a Pyspark (II): Creación de un modelo sencillo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d0fd368-844b-4fde-8f4f-f3e635bd52b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En este notebook (que debes ejecutar en Databricks) vamos a crear un modelo muy sencillo de regresión lineal para que te familiarices en cómo se hace. Verás que el proceso es casi idéntico al realizado con Python con pequeñas matizaciones, algunas no tan pequeñas. Después te animo a que hagas la práctica obligatoria, eso sí con el apoyo de la documentación:\n",
    "\n",
    "Las genéricas (Guia de uso y de APIs, ojo de las funciones y su uso no de API Rest)\n",
    "- https://spark.apache.org/docs/latest/api/python/user_guide/index.html\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/index.html\n",
    "\n",
    "La específica para MLlib:\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86805d0b-7661-4a1a-887f-5f7e84504d10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Objetivo de negocio\n",
    "\n",
    "Tenemos un problema aparentemente sencillo, crea un modelo simple de predicción del salario de una compalía a partir de la experiencia y de la edad de un empleado. Esta vez objetivo y modelo coinciden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcf00227-d094-4b64-b589-1259409088f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Lectura de datos y primer vistazo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "038b9103-616d-4d9e-ab6a-e4477e78ef7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Antes de poder hacer nada, recuerda que en Spark es necesario abrir una sesión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f770538-16f9-4c93-aaac-26ea1370d077",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Modelo').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a63d4196-2c16-4ee6-b977-b4a166aa36b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "491ca809-3c2a-49c1-a500-08f1d9aabacf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Vamos a utilizar una de las tablas o ficheros csv que ya cargamos al comienzo de las sesiones prácticas. Comprobemos primero que sigue aquí:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "026131a4-1a02-4a3c-b1e9-6b677969824b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/FileStore/tables/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1d64a8e-a1df-4962-9487-e5d4da46c78b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ahí está, la test1.csv. Creemos un dataframe con ella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656c3d96-5f19-478b-95be-0dd136f6a96d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training = spark.read.csv(\"dbfs:/FileStore/tables/test1.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ac681d2-2b5a-4265-9496-8e9c5ba70d10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Y echamos un vistazo que en este caso es verla entera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3c81b4-789e-4124-9417-95ace9e7ed48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd3cf8c1-2e77-4574-bd7b-5b12f90b352e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El target es claramente Salary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007d1cc0-775f-4552-903c-b6882c1edf7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target = \"Salary\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a413ff54-7f88-4d1c-845e-db11f2e1d581",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Train-test split, MiniEDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36dcae29-bbf6-4580-876e-f562965beebe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Aunque no tiene mucho sentido por la cantidad de datos hacer un split, veamos como se hace un train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769115e6-efc0-49fa-8cee-3c919b504413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_set, test_set = training.randomSplit([0.65,0.35], seed = 42) # Usamos estos valores de split para que salga algo minimamente usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a5c099-f09b-4121-9d18-018b98134aad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_set.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b44830a-78bb-4adb-92e3-8bdc7224abf2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(test_set.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc82a029-e784-4ada-b27e-21fbb4040b92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Respecto al miniEDA, este dataset no requiere mucho pero aquí se abre un pequeño debate... En Spark no vas a hacer un miniEDA al uso, no al menos con la parte de visualización. En general, si estás usando Spark es que estamos hablando de grandes volúmenes de datos (en absoluto o por unidad de tiempo)... ¿Cuál sería la forma de proceder? Hacer un miniEDA de una muestra representativa del DataFrame de Train.\n",
    "\n",
    "¿Cómo?\n",
    "- Hacer un RandomSplit al 20,80. Quedarse con la parte del 20, pasarla a Pandas y trabajar como hemos venido trabajando hasta aquí. ¿Pasarla a Pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "683ff002-fe57-4bfb-96bf-1495cab397e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Sí, supongamos que train_set es ya nuestra submuestra del train que podemos manejar en memoria en nuestros equipos (fuera del cluster):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "044c32eb-ff4a-4806-855d-8860a995eaae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_minieda = train_set.toPandas()\n",
    "print(type(df_minieda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11898a2d-c16d-418c-807b-0764fab086c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ya es un dataframe pandas, eso quiere decir que no tiene un RDD por debajo, no está particionado y por tanto aunque lo ejecutes en un cluster no saldrá del nodo driver para ser analizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4580fc-d001-4a2e-9242-9fe85c1f78e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_minieda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0922dcc-3500-432d-a61c-2c274fd53028",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Nos quedamos con las dos features que no son el target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fec4b12-e8a4-4091-8f79-defd1f3f7f68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features = [\"age\",\"Experience\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74f7cc75-6c3d-410f-8d24-9ae8f5526c44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Tratamiento y generación de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0f97517-5545-4341-8d5b-759e0c4b7232",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El tratamiento de features hay que hacerlo y para ello tendrás que emplear las funciones tal y como vimos en la parte dedicada a la sintáxis básica (sí tendrás que ver como hacer una estandarizacion, pero ya tienes suficiente capacidad como para verlo en la documentación)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aa39baa-d7de-4df8-aa31-0c8a7b25a77d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**IMPORTANTE:**\n",
    "Pero **la generación de features (su tratamiento final) en Pyspark tiene un punto especial e importante**. Todos los modelos esperan una única columna cuyos valores son un vector con los valores de las features del modelo (y por tanto con el mismo orden para todas las instancias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "295063a6-cc59-42c4-83eb-3d7dbfaea6bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Tendremos que agrupar nuestras variables independientes de forma que queden todas en una columna y dentro de una lista, por lo que crearemos un vector de ensamblaje o \"vector assembler\", de tal modo que queden así esas variables independientes:\n",
    "- [Age, Experience]\n",
    "\n",
    "Lo que haremos con estas dos, será tratarlas como una nueva variable independiente:\n",
    "- [Age, Experience] ----> nueva_variable_independiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf4f8e2d-5561-4b27-925d-eadb49a1f385",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "306b05a1-9461-42f3-abbc-ea4ac1aa6a43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features = \"Independent features\"\n",
    "feature_assembler = VectorAssembler(inputCols=['age', 'Experience'], outputCol= features) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4f2042b-6c50-4156-8f51-b3b738acabe3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Como puedes ver eso no hace nada sobre el dataframe, ahora hay que aplicarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d76d404-0b0d-42fe-afec-c5718a0cb449",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_train = feature_assembler.transform(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03663ae5-414c-40c5-918f-d71c481e6001",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Veremos que se crea una nueva columna cuyos valores se corresponden a unos array con el contenido de aquellas variables independientes que hemos agrupado. Esto será nuestro input feature o lo que solíamos definir como train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "658780d7-2ca7-4362-85b4-7ac0f8c8311a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6778dad-f457-429e-9a36-e8823f09f425",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Seleccionamos las columnas que nos interesan para nuestro modelo: el train (Independent Features) y el test (Salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23802d1-895f-4f5b-a4ab-46f00bd3fff7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "finalized_data_train = output_train.select(features, target)\n",
    "finalized_data_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89a4d3eb-6fff-45a9-9056-c5540d1f9d9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ahora preparamos el test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3fb1cd0-595f-498f-9e3f-43ba2703de91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_test = feature_assembler.transform(test_set) # No te rayes, no estamos normalizando ni haciendo nada que dependa del train por eso se aplica al test directamente\n",
    "finalized_data_test = output_test.select(features, target)\n",
    "finalized_data_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84487a7c-77aa-4f52-a887-1790c378a5d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Instanciación y entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841861f9-2a5e-43f4-9874-6a9f392e4725",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A continuación, entrenaremos un modelo de regresión lineal (fijate que volvemos a la librería ml de pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66a1a99-f3ed-4f08-a857-2541c2662600",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba776c60-c540-4409-8be6-26fcfcbe9639",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "regressor = LinearRegression(featuresCol= features, labelCol= target)\n",
    "regressor = regressor.fit(finalized_data_train) # Al indicarle la labelCol no tengo que deshacerme del target en el train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13e7fe48-1152-4cd7-9f9d-33bbf5316acc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Podemos ver los parámetros que ha entrenado el modelo (es decir los coeficientes de la regresión lineal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bafb0ae-f467-457a-93f5-10319cb5bec9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74646581-c9e5-47ff-b930-658e8e531a58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Asi como el intercetpo (intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed572138-7566-4457-957b-4527f61519ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c983a32f-e036-4d93-9493-e4695d6df69a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Evaluación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e9dd83c-3a75-489a-8687-3d482487350b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Aquí viene otra pequeña diferencia (no tan intensa como la del vector assembler) que es bueno que tengas en cuenta. Creamos las predicciones llamando al metodo evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0709acdf-0676-450a-b74f-d73a653364a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction = regressor.evaluate(finalized_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73309da3-1bd3-4b1b-85f7-65a464762df7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Veamos de qué tipo es esa prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6777657-72bd-4c77-be5b-c3713ba0e6d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5123d3a-3415-4ed6-a136-3886c01f5a6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Es un objeto especial, para ver las predicciones tenemos que bucear un poco más:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a58ce7a-c5b6-44aa-a676-26e482f8346d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b3e0739-39e1-442e-97f2-0aa754599e9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Y a cambio viene con sus métodos para obtener las métricas directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5cbb02b-9862-4772-b0c3-04aa99de8215",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Errores\n",
    "\n",
    "prediction.meanAbsoluteError, prediction.meanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bd1639a-cc3f-4200-9500-5ae1f84f9556",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ya tienes todo lo necesario para seguir profundizando, si quieres, y sobre todo para terminar todo esta unidad de Spark lanzandote a por la práctica obligatoria (que vas a necesitar bucear en la documentación, pero eso es ley de vida en el mundo de la ciencia de datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0466fc1-97d6-47d8-a1ec-3e079c98ee9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_ml_example_databricks",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
