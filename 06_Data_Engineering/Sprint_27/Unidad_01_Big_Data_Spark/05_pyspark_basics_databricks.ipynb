{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a50ca16-fa30-4330-b374-476f016d33d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Introducción a Pyspark (I): Sintáxis básica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71bafe79-41bc-4fe2-87cb-c94f15594489",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El siguiente notebook contiene un pequeño tutorial para explicar los conceptos más básicos de sintaxis de Pyspark que nos permitan luego ver un ejemplo de construcción de modelo y que el alumno pueda profundizar por su cuenta.\n",
    "\n",
    "Como es un notebook preparado para ser ejecutado en \"Databricks\", contiene código que es específico para esta plataforma. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3c8e9ff-2f08-403f-aea5-f72ca16b02ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Inciso DataBrick (I): Crear un cluster y asociar el notebook a dicho clúster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d27df8d5-69bf-47b3-b0b6-daa9cabfcf6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Antes de poder ejecutar cualquier código en Databricks es necesario tener un clúster al que poder asociar dicho código. Si ejectuas la celda que viene a continuación y no has asociado este notebook a un cluster te pedirá hacerlo. En clase vamos a ver cómo crear el cluster por \"fuera\" y luego asociarlo al notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "565f9606-105d-42de-831d-d863860532cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e510a0a-4045-449e-ae6f-1e2ed1b5e069",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Importado pyspark, lo primero como ya comentamos en las sesiones téoricas es crear ese \"contexto\" o \"cursor\" con el que poder interactuar con nuestro clúster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7295e153-0942-4aa1-918a-96083a96b850",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "325b2a8f-4e5f-46cd-893e-8df6ca46f65f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Inciso DataBricks (II): Lectura de tablas y ficheros, empezando por Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d3c4e9d-f23b-4689-85a4-95011c02e11d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Vamos a leer ahora el primer fichero de datos que nos servirá como guía durante estas sesiones introductorias, pero para ello necesitamos saber dónde está (en la sesión anterior lo creamos y vimos el \"path\", pero seguramente no te acuerdas). Para saber en Databricks dónde están las tablas, ejecutamos la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caa0522f-8006-410a-a22a-1f2d6b6b6f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/FileStore/tables/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1421a3af-4d03-4198-9d1d-0c3a19fe9f34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Las tablas están en el sistema de ficheros propio de DataBricks que no es accesible directamente con comandos python (si con pyspark). Como en este tutorial vamos a ir comparando pandas con pyspark, necesitamos mover esas tablas a un directorio \"local\" al nodo driver del cluster y ahí lo leeremos con pandas y luego ya lo volcaremos en un dataframe de spark para ir haciendo la comparativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07274928-5b31-49c2-8116-d275a5ed045f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "NOTA: Al ser copias en el sistema de ficheros local de un nodo del clúster, cuando este se desconecte y termine, se perderán y tendrás que volver a copiarlas del dbfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e13321a7-9460-4562-bab0-fa7782f163f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Las celdas siguiente no la vas a necesitar si programas en pyspark, ojo, es sólo para este tutorial pero ahí te queda por si quieres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "572f0feb-9acb-4907-bd6b-dcd31a2b4c97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs cp dbfs:/FileStore/tables/test1.csv file:/tmp/test1.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fb9a1e5-9245-418e-acf9-d59f3c30bee2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs cp dbfs:/FileStore/tables/test2.csv file:/tmp/test2.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bbacbfd-9178-4a33-ac61-04faf31d731f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs cp dbfs:/FileStore/tables/test3.csv file:/tmp/test3.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a71a166b-60de-435b-be6e-2c5d3ef4ae73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Y ahora ya sí, podemos leer nuestro fichero en pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e76864-7def-49c1-b40c-acaf563b3216",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/tmp/test1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4374b75f-3d28-475f-913b-82d6ee6ea354",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f0d53af-6e02-464d-ac80-75f2609526e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Comenzando con Spark: Lectura de datos y echar un vistazo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "858b1818-25c8-40cb-8b99-af25ca18800b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Como ya se comentó en las sesiones teóricas, lo PRIMERO siempre es abrir un contexto o una sesión con Spark. Hoy en día siempre una sesión porque eso nos evita tener que abrir un contexto para cada módulo de Spark diferente.\n",
    "\n",
    "\n",
    "La sesión adenmás nos da acceso a los métodos y funciones de DataFrame y DataSet, que son los elementos sobre los que trabajaremos de una forma \"parecida\" a como hemos trabajado los Dataframes de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f06707e7-3c68-47b9-8d55-f9e4f495d425",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Practise\").getOrCreate() # Fíjate en la sintaxis, no muy intuitiva. Se le da un nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742c6a31-2dd8-44b8-9860-e6c08c86b365",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aec5dec8-034e-4aa4-a94c-24698f149405",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ahora vamos a leer el fichero a través de la interfaz de sparkm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edadccf4-84dd-48f4-9dc8-026437742f43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv(\"/FileStore/tables/test1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd199033-04a3-4805-a3c7-226a15a4e83c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Recuerda que la lectura anterior es una transformación, no se ha ejecutado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7733b4e6-6725-402a-aac2-4e371a72128c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Por eso ahora lo vamos a forar con el método show() (Que es como el head de pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "834aed34-6d8b-44ff-ad1d-bf81fe76d02e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39e967cc-5ba2-4bc1-9670-fd97b3ee0bd8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "No lee los nombres de las columnas directamente como hace un read_csv porque no le hemos indicado nada al respecto. Ha considerado todas las filas como filas de datos y por eso la primera fila contine el nombre de las columnas. Veamos como forzar la lectura de la primera fila como nombre de columnas y otra forma de hacer la lectura del fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e37d875-58ec-44e5-9722-12974e5c26a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv(\"/FileStore/tables/test1.csv\", header= True)\n",
    "df_pyspark_2 = spark.read.option(\"header\",\"true\").csv(\"/FileStore/tables/test1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a96d061-e3cc-46b0-a4be-8a52677b8b03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d81d6f3b-d5b8-47b6-8460-5dd358c87d0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_pyspark.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0907f1ef-6a44-4f45-b93a-846fcd59fd23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fdce3e2-96a7-4163-afd1-99588d09511b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(df_pyspark_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7adfd9c-95a8-4c1d-8a6b-e512c9920415",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "El comando análogo de info en pandas es printSchema. Ambos nos muestran las columnas, si pueden contener nulos y el tipo de cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8bbb202-8f21-44e1-9635-fd04d3761ebd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d13c6d5-81b5-4ebf-bbd1-6d0870cf1ba8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "facfbb3c-b6d8-41a6-b26b-d71412ac5081",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El atributo dtypes de un DataFrame de Pandas hace algo parecido también:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fccaef6-15be-4e9c-99b5-7cfd87903ab5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.dtypes) # display hace que se vea un poco más estético pero no es necesario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37473e71-f4e6-4861-b0ef-7f43ddb70595",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Pero fijate en la diferencia... En spark todas las columnas son tipo string. A diferencia de pandas, spark si no se lo dices no hace inferencia de tipos. Tenemos que forzarlo como con las cabeceras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be0591f7-f22d-4036-b068-3276ec969447",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_3 = spark.read.csv(\"/FileStore/tables/test1.csv\", header = True, inferSchema= True) # Si no le dices nada a Databricks lee directamente de dbfs con los read de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a194e987-36f2-47e6-9307-e4d58f88a199",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5428d31b-7446-4a57-873f-de905e497745",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb06eac1-2331-47a4-bc95-883edb83d064",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Como puedes ver ahora sí ha cogido los tipos de cada columna. Veamos que también se puede aplicar de la forma alternativa (y ya de paso que podemos leer también del sistema de ficheros locales del nodo driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbb48a04-cd85-4036-a652-50451b090ddb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_4 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"file:/tmp/test1.csv\") # Para acceder en Databricks al sistema de ficheros local de un nodo hay que poner file: delante del path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a409ed-b265-40d7-b927-2f7ccb57461f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "392566d2-f054-46cf-bd6d-54fffe160c05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Los dataframes de Spark también tienen el atributo dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc8de61-ac4f-4c03-a2c6-2e65b188dee0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_3.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ec0ce40-04a9-40f6-9680-339f55d7b30b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para terminar la sesión, completemos el vistazo con el método describe (que existe tanto en Pandas con en Pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6ddab4-916d-4518-9b67-fa40aa689667",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_4.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce06cad4-708d-4f63-824f-5c488d1b89c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Hmmm, no ha hecho nada, describe es una transformación, porque genera otro DataFrame a partir del primero (en este caso \"df_pyspark_4\"), en este caso además se habrá quedado perdida en el limbo porque no hay accion posterior posible que llame al DataFrame resultante (no lo hemos asignado a ninguna variable). En ese sentido es bastante eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4822eb9e-bd54-4179-969c-1ae7c07a67b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_4.describe().show() # Show fuerza a mostrar por pantalla y por tanto a \"obtener\" el dataframe, show es una acción (action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "509253c4-3392-4856-9128-62ed77ae37a7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Aunque, se suele usar show para enseñar una muestra de datos de un dataframe, estos también tienen el método head (aunque su valor por defefcto es 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9783796-6395-47bc-9028-db622f8d50f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6f09648-252a-417f-99b0-8c286e1605a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_3.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccb7a441-23a5-4855-abc3-47a546d4da7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Y vamos bien servidos por ahora. En la siguiente sección/sesión avanzaremos en la selección, indexación y manipulación de columnas de un dataframe Spark. Recuerda que si dejas pasar más de una hora, el cluster se desconectará y para la siguiente sesión tendrás que crear otro nuevo y volver a ejectuar el código hasta aquí. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5dacd0d-103b-470b-bac9-3f574e83056b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0c24cdb-7496-4918-8e25-d8837d93bbfe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Intro Spark (II): Trabajando con columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c98d9a2-5916-469a-8948-a0ccb5ffaab0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Selección de Columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0b1252c-b2c3-4fbf-9563-78bd4d84548b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Empezamos viendo como se selecciona una columna de un dataframe. Primero con pandas y luego con spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf7f7dbe-4b1c-4216-aae7-0c4f593bc0d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df[\"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9daf5554-96ce-48f7-be30-98c6725ef187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_3.select(\"Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "142a525d-d3b8-4cd0-9c36-872fb5806915",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Vuelve a ocurrirnos lo que nos pasaba con el display, select es una transformacion (fíjate además que en Pandas se origina una Serie y en spark se origina un DataFrame) y si no va seguida de un acción relacionada no se ejecuta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "896f7987-3fae-4033-a7f7-25329883d81b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En realidad todavía no ha hecho la operación, está esperando a que \"sea necesario\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86996aaf-9615-414f-bfe4-bd6b27b4e3eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_3.select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0fe590d-5af3-4156-81e5-47aaece31fed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para seleccionar varias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b5501d-2c93-45c8-bdd3-b8b0093ff7b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[[\"Name\",\"Salary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ddb2a48-4fe9-4af9-b338-a96d4b731e66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_3.select([\"Name\",\"Salary\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10b62a6d-dd35-4827-b607-f22c65d1366a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Indexado por posición de columna y fila"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80d9221b-a702-4bbc-b660-6ca6587ef7f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Por indice de columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53ef716c-5a73-4d3b-9eba-ad7e3c056da7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5197579-ed4a-4ee3-bfe7-0952c7f1411b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_3.columns[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2325b23-e4fd-4cff-9f92-270a0767e791",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a55f530-eaf1-402b-a871-8e27560669e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_3.select(df_pyspark_3.columns[:2]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bef1e64-ef87-43c3-88a1-b5be49da3d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "La cosa se \"complica\" cuando queremos trabajar por ejemplo con índices posicionales (porque Pandas tiene siempre un índice posicional implicito), en el caso de Spark si lo necesitamos tenemos que crearlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f86ccd93-fcb0-4662-b1e8-b2ec0b51b98f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[2:4,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd0b6471-7a5d-4b81-bab1-8c6e33beae59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_3.select(df_pyspark_3.columns[:2]).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b8eafde-9217-45d9-ba70-fe53655acc7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Como ves no es lo mismo, para poder hacer algo similar tenemos que crear una columna que contenga un índice hecho \"a mano\". Para ello veamos primero como añadir una columna a un dataframe Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "921ddbba-26c9-42f8-b240-ef412ab502d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creando una nueva columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b82d28d-cdcd-4216-8ead-69c3fdcb34dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7ab645-0e5d-4607-828f-7996a88f5418",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"new_column\"] = df[\"Experience\"] + 2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "214f9e13-bdc1-4197-98b3-c579f97d6dbb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En spark hay que usar el metodo withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd15e6ee-cc6f-4553-b7d8-b886f486fcd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_4 = df_pyspark_4.withColumn(\"new_column\", df_pyspark_4[\"Experience\"] + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10db7e36-17f7-4c09-8f25-0fcdeb96da41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Recuerda que todavía no la ha creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74815ed-a82b-4b27-87d7-41db8f74f10e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6986a73-349a-44f5-a4d9-76084fb275d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c7f10ec-8324-4522-a6ea-1e1638a5bd4a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El show tarda un poco más de lo \"esperable\", precisamente porque es cuando está creando el nuevo dataframe con la columna añadida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fd274ae-3047-4702-a114-81735d50f1a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Cremos ahora un índice y simulemos un iloc (usando filter, un método que veremos un poco más en la siguiente sección/sesión): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a2485cf-2227-4f47-8125-e4e8d8aadb2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df_pyspark_4 = df_pyspark_4.withColumn(\"index\", monotonically_increasing_id() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "660baccf-fd8e-4f80-bbe6-0853d731d4fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ac788c-15e7-4d32-9d02-4421186d6955",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_4.select([\"Name\",\"age\",\"index\"]).filter(df_pyspark_4[\"index\"].between(2,3)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb9ccda7-998d-411d-8355-c41e419e0f0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Pero no teníamos que usar \"select\", ahí pone df_pyspark_4[\"index\"]... Sí pero eso es porque es un objeto columna que no tiene métodos para visualizarse (solo los dataframe lo tienen), pero que sí puede usarse para generar condiciones como veremos cuando tratemos el metodo filter y las formas de seleccionar columnas en un dataframe de spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c790a0-8824-47ab-b933-4e68016578f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(df_pyspark_4[\"index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c2aae9f-70e0-471d-955c-d972a9866b18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Volveremos sobre ello, ahora veamos como eliminar y renombrar columnas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ad73931-de1a-4746-a959-5303bb4f973b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Eliminando columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1636abb-48e1-47f3-a360-b56f072c6f95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.drop(\"new_column\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5da1dff-bdf5-4437-92db-1c80cf1435a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_4.drop(\"Experience\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae8e06dd-aea6-48b0-bce7-c57dfdeddb47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En ambos casos no es una operación que \"mute\" el dataframe original (en Pandas es necesario el argumento inplace y en Spark no es posible hacerlo por filosofía de programación interna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "744f436b-65e6-4f00-9f5a-28d37dc05f83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1948215d-7c87-43a0-91ac-dd1bd3da3f50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Con el renombrado ocurre igual, es necesario o usar el argumento inplace(Pandas) o asignarle el resultado de la operación a una nueva variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "422ba863-403e-46f2-a567-b40b99890531",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.rename(columns={\"Name\": \"Nombre\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3ebfd1-9a80-4f58-a677-e915d2e7641f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e554b93-1f1f-447a-b86a-dcb25a20c7eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_4.withColumnRenamed(\"Name\",\"Nombre\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2d8302f-5e8f-428e-acff-09ce87cf7072",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "253fcc0e-1b1b-4da2-ae76-3d5c8ff78d5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_renombrado = df_pyspark_4.withColumnRenamed(\"Name\",\"Nombre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fda85439-be18-4a08-8651-bd24fb1cb4c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_renombrado_2 = df_renombrado.withColumnRenamed(\"Salary\",\"Nomina\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22139196-5431-48a3-b892-3cdd4b8594a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_renombrado_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc465d46-4850-4826-a7fd-c14310791fab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Aquí terminamos la sección/sesión. En la siguiente veremos el tratamiento de valores faltantes y el filtrado y selección por valores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9e19c0a-f1ba-4516-8e71-07400dec475b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcc8040a-c02a-4809-80c0-239afe6ca2b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Intro Spark (III): Missing Values y Filtrado por valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de04a698-0160-4050-a28f-bb5dce136ac7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73931b26-c759-465b-ace2-49fe5e67ea7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Leemos primero una tabla con valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d2402cb-3c65-43b8-8042-84036c559378",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_5 = spark.read.csv(\"/FileStore/tables/test2.csv\", header = True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1224e37c-0157-4d93-9ce1-fb9160502246",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40b36f06-f998-49c8-8564-c0157b2bb360",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Y aquí las opciones son las de siempre:\n",
    "- Eliminar las filas con valores nulos\n",
    "- Eliminar las columnas con valores nulos\n",
    "- Imputar valores \"representativos\" (moda en categóricas, media en numéricas tipicamente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08e62342-8e8f-4433-a672-3937025c3337",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Eliminando las filas con valores faltantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "935fc151-7d1c-48f0-8263-44a74c9a60bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sin_missing = df_pyspark_5.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06b28d0a-9b44-4978-90c0-c1c942c2abe0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sin_missing.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8300d19f-3abd-4c01-b708-465e2e9ea47f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Quedándonos con las columnas que no tienen missings o faltantes (imagina que \"age\" y \"salary\" no tienen valores faltantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f3c2b45-3795-4963-93de-1eaa86295bbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sin_col_missing = df_pyspark_5.select([\"age\",\"Salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87064a2c-4efd-4f80-94f0-d3396fda1b14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sin_col_missing.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6652fef-ed4e-47ce-a0c8-3131e0e358c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Y antes de pasar a la imputación, veamos algunos de los parámetros de la función drop (parecidos a Pandas):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80eee8bb-b20e-4330-8b42-e3f1bff6e4b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "* how = \"any\" elimina la fila si uno de los valores es nulo (comportamiento por defecto)\n",
    "* how = \"all\" elimina la fila si todos los valores son nulos  \n",
    "* Otro parámetro es \"tresh\". Nos dice el número minimo de valores no nulos que teine que tener una fila antes de eliminarla. Por ejemplo tresh = 2, mantiene las columnas con al menos 2 valores NO NULOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df62f8ae-e801-48ab-b30e-2821b7c548d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_5.na.drop(how = \"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "636c5325-f823-4f2b-8dee-ac03ccb501a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_5.na.drop(how = \"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "846191b0-2534-491f-906d-ad2a69934f35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_5.na.drop(how = \"any\", thresh= 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "996cc0b0-9353-48f9-a593-7341403c2041",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Veamos how en pandas a modo de recordatorio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4571315-002d-47b4-b25a-a36298447ccd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(\"/tmp/test2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f733bbd4-846d-4fd0-aadd-5c3325701dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed5c756a-9c92-4231-a59e-386175c1469f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2.dropna(how = \"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b350490e-4ebd-4753-8399-6cee565bb9c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2.dropna(how = \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ac114a7-4670-4dd9-ab32-bf90001457d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Eliminando valores por columna, cuando sólo queremos eliminar aquellas filas en las que determinadas columnas tienen nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d9c639-9034-4d53-96e3-0d5a84948dd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_5.na.drop(subset=(\"Experience\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d55517b5-025c-4848-959b-ef0096d4fb78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2.dropna(subset= \"Experience\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31a9ba07-7cd2-4f90-8a82-a55696896d08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Y para terminar el tratamiento de nulos, veamos la imputación de missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22dd2098-add0-47cf-aea4-12543ce66510",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_5.na.fill(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad4f764-a947-4380-bcfd-aeba67078d82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_5.na.fill(\"Pepe\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3d275a2-7f7e-4dad-a2dd-e889356012ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Fíjate que sólo imputa en aquellas columnas cuyo tipo es compatible con el valor de relleno. En cambio en Pandas se fuerza la imputación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5b1e27-5be2-4799-87ec-a6dfa83859fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2.fillna(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26444d12-9719-4d71-bb31-6e1125ccd9f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2.Experience.fillna(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7766d8a7-21f0-42e1-be37-a9080cf04e38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Si lo que queremos es imputar valores según la columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a91b8a-a9f3-45bb-8bb5-9dda2889585e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_5.na.fill({\"Name\": \"Ana\", \"Experience\": 3}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22e2a93b-49e9-4763-9457-05302f40e2bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Esta bien, esta bien, hemos imputado valores \"constantes\" o \"fijos\", pero si queremos hacer la imputación con medias o modas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7659371-cdc6-4209-9301-5eb6d9f9a1f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer # Necesitamos tirar de la parte de Machine Learning (el MLLib de PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "971de7c1-9ddd-4ce4-a1fc-d0e70e590575",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "imputer_mean = Imputer(inputCols = [\"age\",\"Experience\",\"Salary\"], outputCols= [\"imputed_age\", \"imputed_Experience\",\"imputed_Salary\"]).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04c1a22-8c86-4bdc-8d99-892981339cc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "imputer_mean.fit(df_pyspark_5).transform(df_pyspark_5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5504552-3621-4530-9dae-7545d15a7d7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Pasemos ahora a filtrar (algo que vimos en la sesión anterior muy de pasada) y a aplicar condiciones (muy parecido a como las aplicamos con Pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f92cd32-28c5-493c-b53c-7dd436c182f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Filtrado y condiciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "227c1766-140b-4dd7-a726-6011bc98dc1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Como vimos cuando intentamos hacer un iloc con dataframes de spark, el método para filtrar es filter y hay que pasarle una condición. De nuevo, filter es una transformación (su resultado es otro dataframe a partir del original) por lo que necesitamos aplicar una acción para forzar la ejecución. \n",
    "\n",
    "Veamos varias formas de proporcionar las condiciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28e9c8e2-4cc1-4114-8885-3efffebc0029",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Como un string usando el nombre de las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02ba2b70-8adb-4310-a0fd-b572823204e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_5.filter(\"Salary <= 20000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b77f8898-210b-4044-84e6-591cb85f3b35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Creando una mascara asignada a una variable, análogamente a como hemos hecho muchas veces con Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec07e87b-d00e-4f1a-9019-a69171e6f6b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "condicion = df_2[\"Experience\"] < 3\n",
    "condicion_spark = df_pyspark_5[\"Experience\"] < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f3cf3d6-57d2-44c0-8d34-8900596c08e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2[condicion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cecbe1a3-155e-4d29-bdde-e190a846322a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_5.filter(condicion_spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1571d9c-51aa-4de8-bb85-72b0e24e1145",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "De igual forma podemos combinar máscaras entre sí con los operadores & (and o y-lógico) y | (or-lógico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc9d0ad8-1ed8-4dc5-a4b7-9a1324c5b999",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "condicion_spark_2 = (condicion_spark) & (df_pyspark_5[\"Salary\"] > 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2bc32b3-ea12-443d-b4b5-33c6d3a12f40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_5.filter(condicion_spark_2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52d109a5-5510-4431-8713-a28cc897b60c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "condicion_2 = (condicion) & (df_2[\"Salary\"] > 10000)\n",
    "df_2[condicion_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ee35036-b4a0-40f5-9fe7-c94f5e5ad5eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Y también tenemos el operador negacion  ~, igual que en pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6db7f6d4-dacd-425d-81d9-cb68f79c4dfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "no_Harsha = ~(df_pyspark_5[\"Name\"] == \"Harsha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cca0cb08-e7bd-4e96-941b-e5080c8c3111",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_5.filter(no_Harsha).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "905208f1-d9bd-4565-8fb4-ce59236a758a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Con esto damos por finalizada la sección. En la siguiente trataremos las agregaciones o agrupaciones (los group by) y la forma de operar sobre ellas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdbfed87-fc9e-4253-91ff-fa66cd3482c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df12efaa-9a3b-47f1-87ad-5348c8b46e09",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Intro Spark (IV): Agrupaciones y agregaciones (groupby)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c5b15a2-193a-420f-9b31-1efd8c746d87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para jugar con las agrupaciones en los dataframes de spark cargamos el tercer juego de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5662e7-04d1-4e51-8ec0-4149afb4fdc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_6 = spark.read.csv(\"/FileStore/tables/test3.csv\", header = True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e03065-c22b-4af0-89e8-71b2928cf0ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7a1dbc-4dc7-49c7-8a51-454693308709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_6.groupby(\"Name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cad592c9-9db9-43df-b57a-d81f0acbdfaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Al igual que con las agrupaciones en Pandas, es necesario aplicar una función de agregación para poder generar un dataframe final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f78ddba8-68f2-4ee2-9171-bd55503b3662",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_grouped = df_pyspark_6.groupBy(\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66801ce7-10b4-465d-beb3-b4158c4da0f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_grouped.sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe756f66-8b70-45ec-ba5e-eb5a5b5171d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Y solo arrastra las columnas sobre las que tiene sentido la función de agregación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faae77cb-40cf-4296-b036-d6a5809664ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_grouped.count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac7941d4-e813-4f61-b63d-c4e7638d39e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Veamos la equivalencia en Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47921df-9fa3-464e-8d06-9a8f40e9a3fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_3 = pd.read_csv(\"/tmp/test3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb3b2b7a-1943-4422-b85c-2940038cefbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b75953f-7cba-4406-9b72-781d77fbb3e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_grouped = df_3.groupby(\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2695c2e0-0680-4bbe-8bf9-c9b647538224",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_grouped.sum(numeric_only= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d1d848f-66bb-4efb-a806-c1296541d5a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Agrupando ahora por departamentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe34241-2e2e-4995-b956-7ec4d6ea8975",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_department = df_pyspark_6.groupBy(\"Departments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f175f658-d21f-4b7b-a98b-6c1c3cc62779",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_department.count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4413f4ed-793f-4c78-85d0-eec7ea78dde6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_department.sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d346aa1e-28c6-408c-9364-4484c49dfa3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_department.mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fdaeed6-3e58-449e-8226-494ca82c78e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Y en pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feaf5f5f-3aef-4067-8c55-68143f55d127",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_departments = df_3.groupby(\"Departments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb15a4d-2f35-4295-a661-636b869dc7d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_departments.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a506ea-d9d4-49d7-8a89-f3b82d0c5077",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_departments.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe13dbd6-e2ec-4fcf-8444-ba6b5bab82cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_departments.mean(numeric_only= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45fdd117-a3f9-473f-aeec-df5e1fd4ed6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### AGGREGATE\n",
    "\n",
    "Al igual que en Pandas, se pueden aplicar diferentes funciones de agregación a una agrupación en función de la columna objetivo. Aunque primero veamos que ocurre cuando lo hacemos sin agrupar (que se aplica a todas las filas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60ac1740-286a-43cf-a4df-ccb568c8ec47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_6.agg({\"Salary\": \"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "432f3196-9893-4a90-9e77-cbecc3380513",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ahora ya sobre la agrupación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7639ac-5c3d-4cd8-aca8-3faff7d82ecb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_6.groupBy(\"Departments\").agg({\"Salary\": \"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9179a14c-72d0-4aff-910b-aef04bb8c379",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Recordando un poco de pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8de1025-ea0a-4cbb-8fb6-0ecf4fdc8bee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd_departments.agg([\"sum\",\"mean\",\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c88ec1b3-c3c5-4fc6-bf62-12fbbcea76d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Un equivalente (mejorado):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b37e6ec1-171a-47aa-a301-b0c68e9e8e4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_6.groupBy(\"Departments\").agg({\"Salary\": \"sum\", \"Salary\": \"mean\", \"Name\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4253d6fe-7afb-49af-bee4-ddbf4bcea793",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Y ya casi estamos preparados para trabajar sobre nuestros dataframes como si fueran pandas, pero para terminar la introducción veremos en la siguiente sección como aplicar funciones a las columnas (y en particular las udf, user defined functions). Pero ya en la siguiente sección/sesión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c259e76-1f7d-44ae-ab3e-2fb19c5ab725",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ad684c9-8613-41e9-8520-cff0ca7cbabe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Intro Spark (V): Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14ff0acb-3764-47ff-9cce-f94b9c07a9c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para empezar nos importamos unas cuantas funciones predefinidas que se pueden aplicar directamente a las columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dec7103-d644-4080-ba92-78c6da022c11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower,col,ascii,mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128532dc-069c-4a3d-bb71-aa62ca7dcbd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c720ace-5b84-446d-be5c-91b334a54633",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Fijate en la sintaxis, que no es precisamente muy directa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c4fa55-3a04-4ca5-b2da-5a8f6f8316da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_6.select(lower(col(\"Name\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a88e2b-f5ec-4f85-8119-5b24fbaa7ca3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pyspark_6.select(lower(df_pyspark_6[\"Name\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54a89249-1983-457c-8769-5ddca0ba8b2e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Es necesario aplicar la función col primero (en el contexto de un método de un dataframe) y luego la función que queramos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d83ccabe-fcaf-4ea1-87f5-d06061273681",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Construimos ahora un dataframe con el código ascii del primer caracter del nombre (sí, no tiene ninguna utilidad aparente pero es que así practicamos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a96ac1b6-b474-48d7-8619-8514d7b8aba4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test = df_pyspark_6.withColumn(\"New_Column\", ascii(col(\"Name\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee6e1a1-06ce-4710-b1be-57a039c232be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2119f49-5736-42c3-ae02-9581fdc01a28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Y ahora algo con más sentido obtener el salario medio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e773014-370a-4237-8548-0f240450677a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "valores = df_pyspark_6.select(mean(col(\"Salary\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daed5d57-9e39-4f7f-a22f-0a1cc7f53fbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71fb5d05-00a1-4997-851b-32ee22b78753",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "valores.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd966b0-a6e6-448d-a092-2a5e6dbf133d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "valores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4a515b6-a07c-4a58-b457-0b81d2074c10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### User-Defined Function (udf): Funciones definidas por el usuario\n",
    "\n",
    "Al igual que usando el método apply en pandas podíamos aplicar una función definida por nosotros mismos a una o varias columnas, una **UDF (User-Defined Function)** en PySpark es una función definida por el usuario que se puede aplicar a columnas de un DataFrame de Spark. Las UDFs permiten realizar operaciones personalizadas en los datos que no están disponibles de forma nativa en las funciones de PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abe77d3d-446c-4fa9-8cb7-3f20b9ce640a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Tienen alguna particularidad como es el registro de la función, por eso vamos a ver paso a paso como crear y usar una udf:\n",
    "1. **Definir la Función en Python**: Escribes una función en Python que realice la operación deseada.\n",
    "2. **Registrar la Función como UDF**: Usas `pyspark.sql.functions.udf` para convertir la función de Python en una UDF.\n",
    "3. **Aplicar la UDF**: Aplicas la UDF a las columnas de un DataFrame de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1517b15e-d89c-4222-bc3e-fcab7d9d0c29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Definimos una función que va a añadir un prefijo \"bootcamp\" a una columna string y a convertirla en 0 si es un número"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "035e793b-3b86-40b6-af94-506a9c7e38eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_test(name):\n",
    "    if type(name) == str:\n",
    "        return f\"bootcamp_{name}\"\n",
    "    else:\n",
    "        return \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9f27232-61a9-499d-8297-6b8b7f6053ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ahora hay que registrarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e322a4ef-9180-4766-9ecc-956ba00396a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Registrar la función como UDF\n",
    "to_test_udf = udf(to_test, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d555e95-1bab-48e1-a0a5-70016fa9fd71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Fijate que le decimos el tipo que retorna la función (Spark está basado en Scala y este lenguaje es mucho más riguroso con los tipos que python y exige este tipo de prevención respecto al tipo de salida, además de que casi todo es inmutable, con cierta flexibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d70a566-e8b1-4cfa-b3d8-302a93484a8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Apliquemos la udf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cdb1216-2dbe-4d66-bbef-564b2b6b995e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aplicar la UDF\n",
    "df_with_upper = df_pyspark_6.withColumn(\"name_upper\", to_test_udf(df_pyspark_6[\"Name\"])) \n",
    "\n",
    "# Mostrar los resultados\n",
    "df_with_upper.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918fbf0f-9aa5-4a3f-9842-e2e7ea90de12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_upper = df_pyspark_6.withColumn(\"salary_upper\", to_test_udf(df_pyspark_6[\"Salary\"]))\n",
    "df_with_upper.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63838a8d-61b6-479c-9f29-22f81634d895",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_upper.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13b92c8d-23c0-4cb5-81eb-a6aa3ad77454",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Y con esto terminamos la sesión introductoria a la sintaxis y al manejo más básico de dataframes Spark. En las siguientes sesiones veremos como construir un modelo supersencillo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9dca82d-9cfb-4620-bd38-c97ea5ccdd1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.stop() # Por ser un poquito ordenado, como nos ocurria con las bases de datos, una vez hemos terminado el trabajo es muy conveniente cerrar las sesiones"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1659062986651242,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_basics_databricks",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c839fd44f6374b9959207347c571eb6142e552910ff4e48977fe7831eb0a6d16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
